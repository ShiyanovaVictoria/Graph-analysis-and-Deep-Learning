{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1. Hand-crafted graph features"
      ],
      "metadata": {
        "id": "FaMjoXkoikjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install grakel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3I8hcz9l2Ga",
        "outputId": "923a8eac-d7d7-4691-8d05-fee62fdf9b85"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grakel in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.26.4)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from grakel) (3.0.11)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.5.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.16.0)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from grakel) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->grakel) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->grakel) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from grakel import GraphKernel"
      ],
      "metadata": {
        "id": "v3GeBPk5lo_t"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Подготовка данных"
      ],
      "metadata": {
        "id": "0XDoJdeqkXv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем набор данных для бинарной классификации графов.\n",
        "Будем использовать графы-пути и графы-циклы"
      ],
      "metadata": {
        "id": "Te9o6NE4kVVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(size = 100, nodes = 30):\n",
        "\n",
        "    graph_list = [nx.cycle_graph(nodes) for i in range(10, size//2 + 10)]\n",
        "    graph_list.extend([nx.path_graph(nodes) for i in range(size//2 + 10, size + 10)])\n",
        "    y = [0 if i < (size//2 + 10) else 1 for i in range(size)]\n",
        "\n",
        "    return graph_list, y"
      ],
      "metadata": {
        "id": "ViIp4A7DNZsL"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Реализация Shortest Path Kernel"
      ],
      "metadata": {
        "id": "Lof5jsRbkd-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для вычисления ядра кратчайших путей на основе векторов, которые содержат количество кратчайших путей различной длины для каждого графа."
      ],
      "metadata": {
        "id": "fN6d5y5hkliG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shortest_path_kernel(graphs_train, graphs_test, max_nodes=30):\n",
        "\n",
        "    # Инициализируем массивы признаков для каждого графа в обучающем и тестовом наборах\n",
        "    features_train = np.zeros((len(graphs_train), max_nodes * (max_nodes - 1)))\n",
        "    features_test = np.zeros((len(graphs_test), max_nodes * (max_nodes - 1)))\n",
        "\n",
        "    # Вычисляем признаки для обучающего набора графов\n",
        "    for index, graph in enumerate(graphs_train):\n",
        "        position = 0  # Позиция в векторе признаков\n",
        "        for start_node in range(graph.number_of_nodes() - 1):\n",
        "            for end_node in range(start_node + 1, graph.number_of_nodes()):\n",
        "                # Рассчитываем кратчайшее расстояние между парами узлов\n",
        "                features_train[index][position] = nx.shortest_path_length(graph, start_node, end_node)\n",
        "                position += 1\n",
        "\n",
        "    # Вычисляем признаки для тестового набора графов\n",
        "    for index, graph in enumerate(graphs_test):\n",
        "        position = 0  # Позиция в векторе признаков\n",
        "        for start_node in range(graph.number_of_nodes() - 1):\n",
        "            for end_node in range(start_node + 1, graph.number_of_nodes()):\n",
        "                # Рассчитываем кратчайшее расстояние между парами узлов\n",
        "                features_test[index][position] = nx.shortest_path_length(graph, start_node, end_node)\n",
        "                position += 1\n",
        "\n",
        "    # Вычисляем матрицы ядра через скалярное произведение признаков\n",
        "    kernel_train = np.dot(features_train, features_train.T)\n",
        "    kernel_test = np.dot(features_test, features_train.T)\n",
        "\n",
        "    return kernel_train, kernel_test"
      ],
      "metadata": {
        "id": "ZnLc-Uvb1eAV"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Обучение модели SVC"
      ],
      "metadata": {
        "id": "8BBoFyRDko1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_list, y = create_dataset(1000)\n",
        "train_graphs, test_graphs, y_train, y_test = train_test_split(graph_list, y, test_size=0.1, stratify=y)\n",
        "\n",
        "K_train_gk, K_test_gk = shortest_path_kernel(train_graphs, test_graphs)\n",
        "\n",
        "svc = SVC(kernel='precomputed', random_state = 42)\n",
        "\n",
        "param_grid = {'C': [1, 10, 100], 'tol': [0.01, 0.001, 0.0001]}\n",
        "grid_search = GridSearchCV(svc, param_grid)\n",
        "grid_search.fit(K_train_gk, y_train)\n",
        "\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sddg4UUX83SS",
        "outputId": "7e4f1281-8c00-4441-ff99-d04d51b961f4"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1, 'tol': 0.01}"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = SVC(C = 1, kernel ='precomputed', tol = 0.01, random_state = 42)\n",
        "\n",
        "best_model.fit(K_train_gk, y_train)\n",
        "y_pred = best_model.predict(K_test_gk)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28U-XDhInYsu",
        "outputId": "0d865989-c9e4-4bd4-e5d1-2d0ea1d67390"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98        51\n",
            "           1       0.96      1.00      0.98        49\n",
            "\n",
            "    accuracy                           0.98       100\n",
            "   macro avg       0.98      0.98      0.98       100\n",
            "weighted avg       0.98      0.98      0.98       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Реализация Weisfeiler-Lehman Kernel"
      ],
      "metadata": {
        "id": "tlp0yFpQkuew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь реализуем ядро Вайсфейлера-Лемана (WL Kernel), который сравнивает графы на основе их структурных инвариантов, построенных с использованием метода хеширования меток узлов."
      ],
      "metadata": {
        "id": "MGViwdwKkv41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weisfeiler_lehman_kernel(train_graphs, test_graphs, iterations=30):\n",
        "\n",
        "    # Инициализация векторов признаков для тренировочных и тестовых графов\n",
        "    phi_train = []\n",
        "    phi_test = []\n",
        "\n",
        "    # Проход по каждому графу из обучающего набора\n",
        "    for i, graph in enumerate(train_graphs):\n",
        "        phi_train_dict = {1: graph.number_of_nodes()}  # Словарь признаков (ключ - метка, значение - количество узлов с данной меткой)\n",
        "        hash_map = {'1': '1'}  # Хэш-таблица для хранения уникальных меток\n",
        "        node_labels = {node: 1 for node in graph.nodes()}  # Начальная метка для всех узлов\n",
        "\n",
        "        # Цикл по количеству итераций WL\n",
        "        for it in range(iterations):\n",
        "            # Добавляем дополнительные ячейки в словарь признаков для данной итерации\n",
        "            for extra in range(it * 10):\n",
        "                phi_train_dict[extra + 2] = 0\n",
        "\n",
        "            # Временный словарь для обновленных меток узлов на текущей итерации\n",
        "            temp_hash = {}\n",
        "\n",
        "            # Обновляем метки узлов с учетом меток их соседей\n",
        "            for node in graph.nodes():\n",
        "                # Формируем строку меток текущего узла и его соседей\n",
        "                neighbor_labels = ''.join(sorted(str(node_labels[neighbor]) for neighbor in graph.neighbors(node)))\n",
        "                temp_label = f\"{node_labels[node]},{neighbor_labels}\"\n",
        "                temp_hash[node] = temp_label\n",
        "\n",
        "            # Обновляем глобальные метки, если встретились новые комбинации\n",
        "            for node in sorted(temp_hash, key=lambda k: temp_hash[k]):\n",
        "                if temp_hash[node] not in hash_map.values():\n",
        "                    max_key = len(hash_map) + 1\n",
        "                    hash_map[max_key] = temp_hash[node]\n",
        "\n",
        "            # Обновляем метки узлов в словаре node_labels\n",
        "            for node, label in temp_hash.items():\n",
        "                new_label = next(k for k, v in hash_map.items() if v == label)\n",
        "                node_labels[node] = new_label\n",
        "                phi_train_dict[new_label] = phi_train_dict.get(new_label, 0) + 1\n",
        "\n",
        "        # Сортировка признаков и преобразование в вектор\n",
        "        sorted_features = sorted(phi_train_dict.items())\n",
        "        feature_vector = np.array([count for _, count in sorted_features])\n",
        "        phi_train.append(feature_vector)\n",
        "\n",
        "    # Аналогичные вычисления для тестовых графов\n",
        "    for i, graph in enumerate(test_graphs):\n",
        "        phi_test_dict = {1: graph.number_of_nodes()}\n",
        "        hash_map = {'1': '1'}\n",
        "        node_labels = {node: 1 for node in graph.nodes()}\n",
        "\n",
        "        for it in range(iterations):\n",
        "            for extra in range(it * 10):\n",
        "                phi_test_dict[extra + 2] = 0\n",
        "\n",
        "            temp_hash = {}\n",
        "\n",
        "            for node in graph.nodes():\n",
        "                neighbor_labels = ''.join(sorted(str(node_labels[neighbor]) for neighbor in graph.neighbors(node)))\n",
        "                temp_label = f\"{node_labels[node]},{neighbor_labels}\"\n",
        "                temp_hash[node] = temp_label\n",
        "\n",
        "            for node in sorted(temp_hash, key=lambda k: temp_hash[k]):\n",
        "                if temp_hash[node] not in hash_map.values():\n",
        "                    max_key = len(hash_map) + 1\n",
        "                    hash_map[max_key] = temp_hash[node]\n",
        "\n",
        "            for node, label in temp_hash.items():\n",
        "                new_label = next(k for k, v in hash_map.items() if v == label)\n",
        "                node_labels[node] = new_label\n",
        "                phi_test_dict[new_label] = phi_test_dict.get(new_label, 0) + 1\n",
        "\n",
        "        sorted_features = sorted(phi_test_dict.items())\n",
        "        feature_vector = np.array([count for _, count in sorted_features])\n",
        "        phi_test.append(feature_vector)\n",
        "\n",
        "    # Вычисление матриц ядровых функций\n",
        "    K_train = np.dot(phi_train, np.array(phi_train).T)\n",
        "    K_test = np.dot(phi_test, np.array(phi_train).T)\n",
        "\n",
        "    return K_train, K_test"
      ],
      "metadata": {
        "id": "qXy5lkq94BBD"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_list, y = create_dataset(1000)\n",
        "\n",
        "train_graphs, test_graphs, y_train, y_test = train_test_split(graph_list, y, test_size=0.1, stratify=y)\n",
        "K_train_gk2, K_test_gk2 = weisfeiler_lehman_kernel(train_graphs, test_graphs, 10)\n",
        "\n",
        "svc2 = SVC(C = 1, kernel ='precomputed', tol = 0.01, random_state = 42)\n",
        "\n",
        "param_grid = {'C': [1, 10, 100], 'tol': [0.01, 0.001, 0.0001]}\n",
        "grid_search = GridSearchCV(svc2, param_grid)\n",
        "grid_search.fit(K_train_gk, y_train)\n",
        "\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8OiCryHFitQ",
        "outputId": "bfcb8b6c-4e2e-4748-cbbe-92e291319c3a"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1, 'tol': 0.01}"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2 = SVC(C = 1, kernel ='precomputed', tol = 0.01, random_state = 42)\n",
        "\n",
        "best_model2.fit(K_train_gk2, y_train)\n",
        "y_pred2 = best_model2.predict(K_test_gk2)\n",
        "\n",
        "print(classification_report(y_test, y_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5h41BfTFy0F",
        "outputId": "bd0e4351-cd94-4483-e19a-0ab387e15765"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97        51\n",
            "           1       0.94      1.00      0.97        49\n",
            "\n",
            "    accuracy                           0.97       100\n",
            "   macro avg       0.97      0.97      0.97       100\n",
            "weighted avg       0.97      0.97      0.97       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `Weisfeiler-Lehman kernel` выполняется значительно быстрее, так как он работает с метками узлов и использует итеративное обновление этих меток для захвата структурной информации. В отличие от этого, `shortest_path_kernel()` требует вычислений кратчайших путей для всех пар узлов, что значительно увеличивает время работы для больших графов.\n",
        "\n",
        "3. `shortest_path_kernel()` более точный, поскольку сохраняет более детальную информацию о графе, так как учитывает расстояния между всеми узлами. В `Weisfeiler-Lehman kernel` информация о графе представлена более абстрактно через метки узлов, что снижает точность по сравнению с кратчайшими путями.\n",
        "\n",
        "В целом, `Weisfeiler-Lehman kernel` является хорошим выбором для быстрого анализа крупных графов с умеренной детализацией, тогда как `shortest_path_kernel()` может быть полезен, если требуется максимальная точность и графы небольшие."
      ],
      "metadata": {
        "id": "DBOe59znC_MD"
      }
    }
  ]
}